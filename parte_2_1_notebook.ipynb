{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c79557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"noticias_unificadas.tsv\",\n",
    "    encoding=\"utf-8\",\n",
    "    sep=\"\\t\",\n",
    "    dtype={\"fecha\": \"string\", \"titulo\": \"string\", \"contenido\": \"string\", \"seccion\": \"string\", \"link\": \"string\"},\n",
    "    quoting=0,\n",
    "    na_filter=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d13127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de t√≥picos √∫nicos: 7\n"
     ]
    }
   ],
   "source": [
    "topicos = df['seccion'].value_counts()\n",
    "print(f\"\\nTotal de t√≥picos √∫nicos: {df['seccion'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe3cd776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha</th>\n",
       "      <th>titulo</th>\n",
       "      <th>contenido</th>\n",
       "      <th>seccion</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>Jueces rechazan intento de afectaci√≥n a la ind...</td>\n",
       "      <td>Desde la ciudad de Tacna, jueces y juezas de t...</td>\n",
       "      <td>Pol√≠tica</td>\n",
       "      <td>https://diariocorreo.pe/politica/jueces-rechaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>Liga 1: Lo gritan los ‚ÄúChurres‚Äù y todo el pueb...</td>\n",
       "      <td>Alianza Atl√©tico le sac√≥ lustre a su clasifica...</td>\n",
       "      <td>Deportes</td>\n",
       "      <td>https://diariocorreo.pe/deportes/alianza-atlet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>Proponen sancionar con hasta 10 a√±os de c√°rcel...</td>\n",
       "      <td>La congresista Elizabeth Medina Hermosillo, de...</td>\n",
       "      <td>Pol√≠tica</td>\n",
       "      <td>https://diariocorreo.pe/politica/proponen-sanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>Este lunes inicia la semana de representaci√≥n ...</td>\n",
       "      <td>Desde este lunes 10 hasta el viernes 14 de nov...</td>\n",
       "      <td>Pol√≠tica</td>\n",
       "      <td>https://diariocorreo.pe/politica/este-lunes-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-09</td>\n",
       "      <td>Selecci√≥n peruana eval√∫a reprogramaci√≥n de par...</td>\n",
       "      <td>La Federaci√≥n Peruana de F√∫tbol (FPF) inform√≥ ...</td>\n",
       "      <td>Deportes</td>\n",
       "      <td>https://diariocorreo.pe/deportes/seleccion-per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fecha                                             titulo  \\\n",
       "0  2025-11-09  Jueces rechazan intento de afectaci√≥n a la ind...   \n",
       "1  2025-11-09  Liga 1: Lo gritan los ‚ÄúChurres‚Äù y todo el pueb...   \n",
       "2  2025-11-09  Proponen sancionar con hasta 10 a√±os de c√°rcel...   \n",
       "3  2025-11-09  Este lunes inicia la semana de representaci√≥n ...   \n",
       "4  2025-11-09  Selecci√≥n peruana eval√∫a reprogramaci√≥n de par...   \n",
       "\n",
       "                                           contenido   seccion  \\\n",
       "0  Desde la ciudad de Tacna, jueces y juezas de t...  Pol√≠tica   \n",
       "1  Alianza Atl√©tico le sac√≥ lustre a su clasifica...  Deportes   \n",
       "2  La congresista Elizabeth Medina Hermosillo, de...  Pol√≠tica   \n",
       "3  Desde este lunes 10 hasta el viernes 14 de nov...  Pol√≠tica   \n",
       "4  La Federaci√≥n Peruana de F√∫tbol (FPF) inform√≥ ...  Deportes   \n",
       "\n",
       "                                                link  \n",
       "0  https://diariocorreo.pe/politica/jueces-rechaz...  \n",
       "1  https://diariocorreo.pe/deportes/alianza-atlet...  \n",
       "2  https://diariocorreo.pe/politica/proponen-sanc...  \n",
       "3  https://diariocorreo.pe/politica/este-lunes-in...  \n",
       "4  https://diariocorreo.pe/deportes/seleccion-per...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4fcf6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import trigrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa87a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/joelibaceta/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/joelibaceta/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d18acfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/home/joelibaceta/nltk_data/tokenizers/punkt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find(\"tokenizers/punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d731cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/home/joelibaceta/nltk_data/tokenizers/punkt_tab')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find(\"tokenizers/punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1ccf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def tokenize_es(text: str) -> list[list[str]]:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    sentences = sent_tokenize(text, language=\"spanish\")\n",
    "    tokenized = []\n",
    "    for s in sentences:\n",
    "        toks = word_tokenize(s, language=\"spanish\")\n",
    "        # lower only alphabetic tokens, keep punctuation as-is\n",
    "        toks = [t.lower() if t.isalpha() else t for t in toks]\n",
    "        if toks:\n",
    "            tokenized.append(toks)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1ced0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def load_corpus(df: pd.DataFrame, by_category: bool = False) -> Dict[str, List[List[str]]]:\n",
    "    buckets: Dict[str, List[List[str]]] = defaultdict(list)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        categoria = (row.get(\"seccion\") or \"\").strip()\n",
    "        noticia = row.get(\"contenido\")\n",
    "        \n",
    "        if not noticia or not isinstance(noticia, str):\n",
    "            continue\n",
    "            \n",
    "        sents_toks = tokenize_es(noticia)\n",
    "        \n",
    "        if by_category and categoria:\n",
    "            for s in sents_toks:\n",
    "                if s:\n",
    "                    buckets[categoria].append(s)\n",
    "        else:\n",
    "            for s in sents_toks:\n",
    "                if s:\n",
    "                    buckets[\"_GLOBAL\"].append(s)\n",
    "    \n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52213eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_por_categoria = load_corpus(df, by_category=True)\n",
    "corpus_global = load_corpus(df, by_category=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5cfedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trigrams(tokenized_sentences: List[List[str]]):\n",
    "    \"\"\"\n",
    "      model[(w1,w2)][w3] = prob\n",
    "    \"\"\"\n",
    "    model = defaultdict(lambda: defaultdict(float))\n",
    "    for sent in tokenized_sentences:\n",
    "        for w1, w2, w3 in trigrams(sent, pad_left=True, pad_right=True):\n",
    "            model[(w1, w2)][w3] += 1.0\n",
    "\n",
    "    for w1w2 in model:\n",
    "        total = sum(model[w1w2].values())\n",
    "        if total > 0:\n",
    "            for w3 in model[w1w2]:\n",
    "                model[w1w2][w3] /= total\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c23821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274244"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_global = train_trigrams(corpus_global[\"_GLOBAL\"])\n",
    "len(model_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0aefaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo para: Pol√≠tica (164273 oraciones)\n",
      "Entrenando modelo para: Deportes (64217 oraciones)\n",
      "Entrenando modelo para: Deportes (64217 oraciones)\n",
      "Entrenando modelo para: Espect√°culos (95589 oraciones)\n",
      "Entrenando modelo para: Espect√°culos (95589 oraciones)\n",
      "Entrenando modelo para: Cultura (82146 oraciones)\n",
      "Entrenando modelo para: Cultura (82146 oraciones)\n",
      "Entrenando modelo para: Econom√≠a (56308 oraciones)\n",
      "Entrenando modelo para: Econom√≠a (56308 oraciones)\n",
      "Entrenando modelo para: Mundo (81657 oraciones)\n",
      "Entrenando modelo para: Mundo (81657 oraciones)\n",
      "Entrenando modelo para: Policiales (50687 oraciones)\n",
      "Entrenando modelo para: Policiales (50687 oraciones)\n"
     ]
    }
   ],
   "source": [
    "models_por_categoria = {}\n",
    "\n",
    "for categoria, oraciones in corpus_por_categoria.items():\n",
    "    print(f\"Entrenando modelo para: {categoria} ({len(oraciones)} oraciones)\")\n",
    "    models_por_categoria[categoria] = train_trigrams(oraciones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c7e5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import random\n",
    "\n",
    "def sample_next(model: dict, w1: str, w2: str) -> Optional[str]:\n",
    "    dist = model.get((w1, w2), {})\n",
    "    # Heuristica por si no hay distribuci√≥n para (w1, w2)\n",
    "    if not dist:\n",
    "        # Fallback 1: Buscar todos los contextos que terminen en w2\n",
    "        for key in model.keys():\n",
    "            if key[1] == w2 and model[key]:\n",
    "                dist = model[key]\n",
    "                break\n",
    "        # Fallback 2: Si a√∫n no hay nada, usar inicio de oraci√≥n\n",
    "        if not dist:\n",
    "            dist = model.get((None, None), {})\n",
    "        # Fallback 3: Si todav√≠a no hay nada, elegir un contexto aleatorio\n",
    "        if not dist and model:\n",
    "            random_key = random.choice(list(model.keys()))\n",
    "            dist = model[random_key]\n",
    "\n",
    "    r = random.random()\n",
    "    acc = 0.0\n",
    "    for w3, p in dist.items():\n",
    "        acc += p\n",
    "        if acc >= r:\n",
    "            return w3\n",
    "    \n",
    "    return list(dist.keys())[-1] if dist else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3586a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model: dict, seeds: Tuple[str, str] = (None, None), max_len: int = 30) -> str:\n",
    "    text: List[str] = [seeds[0], seeds[1]]\n",
    "    sentence_finished = False\n",
    "\n",
    "    while not sentence_finished and len(text) < max_len + 2:\n",
    "        w3 = sample_next(model, text[-2], text[-1])\n",
    "        text.append(w3)\n",
    "        if text[-2:] == [None, None] or w3 is None:\n",
    "            sentence_finished = True\n",
    "\n",
    "    sentence = \" \".join([t for t in text if t])\n",
    "    sentence = sentence.strip()\n",
    "    if sentence and sentence[-1] not in \".!?\":\n",
    "        sentence += \".\"\n",
    "    if sentence:\n",
    "        sentence = sentence[0].upper() + sentence[1:]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92ae162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paragraph(model: dict, n_sentences: int = 3, seeds: Tuple[str, str] = (None, None)) -> str:\n",
    "    return \" \".join(generate_sentence(model, seeds=seeds) for _ in range(n_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62c0af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La Policia aprovecha que estamos descuidando personal para satisfacci√≥n m√≠a pero trae como estigma el haber influido en la zona afectada . La Policia durante protestas impide la inscripci√≥n de alianzas proceder√° hasta los colectiveros y el fondo editorial .\n"
     ]
    }
   ],
   "source": [
    "print(generate_paragraph(model_global, n_sentences=2, seeds=(\"La\", \"Policia\", )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1e17e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El equipo que se proclam√≥ m√°ximo goleador hist√≥rico de cerro colorado y Arizona.6:30 p.m . El equipo piurano en otros resultados : los chankas , amel√≠ fue enf√°tico al asegurar que protegeremos a cada continente en el equipo de v√≥ley 2025.¬°V√≥ley peruano‚Ä¶ v√≥ley peruano‚Ä¶ !\n"
     ]
    }
   ],
   "source": [
    "print(generate_paragraph(models_por_categoria[\"Deportes\"], n_sentences=2, seeds=(\"El\", \"equipo\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25399f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POL√çTICA:\n",
      "El presidente pero vizcarra lider√≥ red criminalfiscal√≠a pide prisi√≥n preventiva de andr√©s manuel l√≥pez obrador y continuada desde hace mucho tiempo han criticado la lejan√≠a del local del canal de youtube del. El presidente pero vizcarra lider√≥ red criminalfiscal√≠a pide prisi√≥n preventiva en su derecho al voto de los internos e internas y afirm√≥ que si ello no cumpli√≥ con las reglas de conducta.El.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPOL√çTICA:\")\n",
    "print(generate_paragraph(models_por_categoria[\"Pol√≠tica\"], n_sentences=2, seeds=(\"El\", \"presidente\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9d12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import format_sentence\n",
    "\n",
    "def generate_sentence(model: dict, seeds: Tuple[str, str] = (None, None), max_len: int = 30) -> str:\n",
    "    w1 = seeds[0].lower() if seeds[0] and seeds[0] is not None else None\n",
    "    w2 = seeds[1].lower() if seeds[1] and seeds[1] is not None else None\n",
    "    \n",
    "    text: List[str] = []\n",
    "\n",
    "    if w1 is not None:\n",
    "        text.append(w1)\n",
    "    if w2 is not None:\n",
    "        text.append(w2)\n",
    "    \n",
    "    if not text:\n",
    "        w1, w2 = None, None\n",
    "    elif len(text) == 1:\n",
    "        w1, w2 = None, text[0]\n",
    "    else:\n",
    "        w1, w2 = text[-2] if len(text) >= 2 else None, text[-1]\n",
    "    \n",
    "    sentence_finished = False\n",
    "    iterations_without_word = 0\n",
    "    \n",
    "    while not sentence_finished and len(text) < max_len:\n",
    "        w3 = sample_next(model, w1, w2)\n",
    "        if w3 is None:\n",
    "            iterations_without_word += 1\n",
    "            if iterations_without_word > 3:\n",
    "                sentence_finished = True\n",
    "            w1, w2 = None, None\n",
    "            continue\n",
    "        \n",
    "        iterations_without_word = 0\n",
    "        text.append(w3)\n",
    "        \n",
    "        w1, w2 = w2, w3\n",
    "\n",
    "        if w3 in ['.', '!', '?']:\n",
    "            sentence_finished = True\n",
    "    \n",
    "    return format_sentence(text, capitalize_first=True, add_final_punct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9ba95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paragraph(model: dict, n_sentences: int = 3, seeds: Tuple[str, str] = (None, None)) -> str:\n",
    "    sentences = []\n",
    "    sentences.append(generate_sentence(model, seeds=seeds))\n",
    "    for _ in range(n_sentences - 1):\n",
    "        sentences.append(generate_sentence(model, seeds=(None, None)))\n",
    "    return \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f39ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayer sucedio mientras el apra fue proscrito con la arcilla suiza con parciales de hasta S/20 mil al mes los aranceles, que parte de la mujer, fanny montellanos. Adem√°s de declamar, cantar y bailar. He pedido que el s√°bado a la remisi√≥n de la separaci√≥n con un mensaje por la noche del mi√©rcoles en culiac√°n, en un ambiente positivo y comenc√© el viaje.\n"
     ]
    }
   ],
   "source": [
    "print(generate_paragraph(model_global, n_sentences=3, seeds=(\"Ayer\", \"sucedio\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec5f00f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEPORTES - Seeds: ('El', 'equipo')\n",
      "El equipo que amo, estoy muy ilusionado. En ese contexto, los comisarios `` de aqu√≠ a las 8:00 p.m. Si bien los primeros dos partidos de septiembre con un medio ecuatoriano. Video recomendado: el t√©cnico gerardo ameli, quien tambi√©n se unieron para mostrar que hay seguridad ''.\n"
     ]
    }
   ],
   "source": [
    "print(\"DEPORTES - Seeds: ('El', 'equipo')\")\n",
    "print(generate_paragraph(models_por_categoria[\"Deportes\"], n_sentences=4, seeds=(\"El\", \"equipo\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46dc13e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POL√çTICA - Seeds: ('El', 'presidente')\n",
      "El presidente de china en per√∫: echa muni cae en saco roto: ni c√°rceles, cometi√≥ un error involuntario ‚Äù porque la cantidad de nombramientos en Cajamarca.Juan sheput. Ya se cobra por ello, creo, sin embargo, el comunicado emitido por el poder judicial, ministerio de econom√≠a y finanzas, jos√© cevasco denuncia falta de. Pero si conversar no es ciega; qu√© dis√≠mil puede ser detenido en roma, tom√≥ asiento, y se incautaron y destruyeron veredas ‚Äîcer√°micas de piso tipo roca‚Äî para.\n"
     ]
    }
   ],
   "source": [
    "print(\"POL√çTICA - Seeds: ('El', 'presidente')\")\n",
    "print(generate_paragraph(models_por_categoria[\"Pol√≠tica\"], n_sentences=3, seeds=(\"El\", \"presidente\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6bab921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELO GLOBAL - Sin seeds (inicio natural)\n",
      "Asimismo cree firmemente que s√≠ exist√≠a ‚Äú un mill√≥n de d√≥lares en efectivo, cheque o anotaci√≥n contable relacionados con pol√≠ticas de emergencia al hospital luis s√°enz recibi√≥ a universitario. Estas organizaciones de derechos humanos ( cidh ). Con esta persona no puede decir ning√∫n peruano, consolidando una alianza militar con un equipo de ‚Äò leyenda viva ‚Äô jorge nieto montesinos.\n"
     ]
    }
   ],
   "source": [
    "print(\"MODELO GLOBAL - Sin seeds (inicio natural)\")\n",
    "print(generate_paragraph(model_global, n_sentences=3, seeds=(None, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb861da",
   "metadata": {},
   "source": [
    "## üîç Validaci√≥n del Modelo\n",
    "\n",
    "Verificamos que el modelo funcione correctamente con diferentes casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d284236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRUEBAS DE VALIDACI√ìN DEL MODELO DE TRIGRAMAS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Verificando estructura del modelo...\n",
      "   - Contextos √∫nicos en modelo global: 2,274,244\n",
      "   - Ejemplo de contexto: [(None, None), (None, 'desde'), ('desde', 'la')]\n",
      "   - Suma de probabilidades para contexto (None, None): 1.0000\n",
      "   - ‚úÖ Probabilidades correctas\n",
      "\n",
      "2Ô∏è‚É£ Probando generaci√≥n con diferentes seeds...\n",
      "   Seeds (        el, presidente): El presidente en funciones....\n",
      "   Seeds (        la,    polic√≠a): La polic√≠a nacional del per√∫?...\n",
      "   Seeds (       los,  ministros): Los ministros de estado de bah√≠a blanca, trump ya hab√≠a sido interrogado por la ...\n",
      "   Seeds (      None,       None): Per√∫21 epaper....\n",
      "\n",
      "3Ô∏è‚É£ Probando diferentes longitudes...\n",
      "   Max length=10: gener√≥ 6 palabras\n",
      "   Max length=20: gener√≥ 20 palabras\n",
      "   Max length=30: gener√≥ 4 palabras\n",
      "\n",
      "4Ô∏è‚É£ Verificando variedad en generaciones...\n",
      "   Generadas 5 oraciones √∫nicas de 5 intentos\n",
      "   ‚úÖ Buena variedad\n",
      "\n",
      "================================================================================\n",
      "‚úÖ VALIDACI√ìN COMPLETADA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PRUEBAS DE VALIDACI√ìN DEL MODELO DE TRIGRAMAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test 1: Verificar estructura del modelo\n",
    "print(\"\\n1Ô∏è‚É£ Verificando estructura del modelo...\")\n",
    "print(f\"   - Contextos √∫nicos en modelo global: {len(model_global):,}\")\n",
    "print(f\"   - Ejemplo de contexto: {list(model_global.keys())[:3]}\")\n",
    "\n",
    "# Verificar que las probabilidades sumen 1\n",
    "sample_context = list(model_global.keys())[0]\n",
    "total_prob = sum(model_global[sample_context].values())\n",
    "print(f\"   - Suma de probabilidades para contexto {sample_context}: {total_prob:.4f}\")\n",
    "print(f\"   - ‚úÖ Probabilidades correctas\" if abs(total_prob - 1.0) < 0.001 else f\"   - ‚ö†Ô∏è Error en probabilidades\")\n",
    "\n",
    "# Test 2: Verificar que los seeds funcionan\n",
    "print(\"\\n2Ô∏è‚É£ Probando generaci√≥n con diferentes seeds...\")\n",
    "test_seeds = [\n",
    "    (\"el\", \"presidente\"),\n",
    "    (\"la\", \"polic√≠a\"),\n",
    "    (\"los\", \"ministros\"),\n",
    "    (None, None),  # Sin seeds\n",
    "]\n",
    "\n",
    "for w1, w2 in test_seeds:\n",
    "    try:\n",
    "        result = generate_sentence(model_global, seeds=(w1, w2), max_len=20)\n",
    "        w1_str = w1 if w1 else \"None\"\n",
    "        w2_str = w2 if w2 else \"None\"\n",
    "        print(f\"   Seeds ({w1_str:>10}, {w2_str:>10}): {result[:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Seeds ({w1}, {w2}): ‚ùå Error: {e}\")\n",
    "\n",
    "# Test 3: Verificar longitudes\n",
    "print(\"\\n3Ô∏è‚É£ Probando diferentes longitudes...\")\n",
    "for length in [10, 20, 30]:\n",
    "    result = generate_sentence(model_global, seeds=(None, None), max_len=length)\n",
    "    word_count = len(result.split())\n",
    "    print(f\"   Max length={length}: gener√≥ {word_count} palabras\")\n",
    "\n",
    "# Test 4: Verificar consistencia (m√∫ltiples generaciones)\n",
    "print(\"\\n4Ô∏è‚É£ Verificando variedad en generaciones...\")\n",
    "generations = set()\n",
    "for _ in range(5):\n",
    "    result = generate_sentence(model_global, seeds=(\"el\", \"gobierno\"), max_len=15)\n",
    "    generations.add(result)\n",
    "\n",
    "print(f\"   Generadas {len(generations)} oraciones √∫nicas de 5 intentos\")\n",
    "print(f\"   {'‚úÖ Buena variedad' if len(generations) >= 3 else '‚ö†Ô∏è Poca variedad - posible problema'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ VALIDACI√ìN COMPLETADA\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7108de3",
   "metadata": {},
   "source": [
    "### üîß Diagn√≥stico de Problemas Comunes\n",
    "\n",
    "Analicemos posibles problemas en el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cba2c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç AN√ÅLISIS DE CONTEXTOS CON PADDING (None)\n",
      "======================================================================\n",
      "Total de contextos con None: 23262\n",
      "Contextos de inicio (None, None): 21988\n",
      "\n",
      "Palabras que pueden iniciar una oraci√≥n:\n",
      "                el: 0.0787\n",
      "                la: 0.0602\n",
      "                en: 0.0454\n",
      "             video: 0.0378\n",
      "                 ‚Äú: 0.0354\n",
      "                ``: 0.0322\n",
      "         aprovecha: 0.0310\n",
      "            Per√∫21: 0.0225\n",
      "          b√∫scanos: 0.0196\n",
      "            ¬°Ahora: 0.0196\n",
      "\n",
      "üîç AN√ÅLISIS DE MAY√öSCULAS/MIN√öSCULAS\n",
      "======================================================================\n",
      "Total de contextos con None: 23262\n",
      "Contextos de inicio (None, None): 21988\n",
      "\n",
      "Palabras que pueden iniciar una oraci√≥n:\n",
      "                el: 0.0787\n",
      "                la: 0.0602\n",
      "                en: 0.0454\n",
      "             video: 0.0378\n",
      "                 ‚Äú: 0.0354\n",
      "                ``: 0.0322\n",
      "         aprovecha: 0.0310\n",
      "            Per√∫21: 0.0225\n",
      "          b√∫scanos: 0.0196\n",
      "            ¬°Ahora: 0.0196\n",
      "\n",
      "üîç AN√ÅLISIS DE MAY√öSCULAS/MIN√öSCULAS\n",
      "======================================================================\n",
      "Contextos con may√∫sculas: 60099\n",
      "‚ö†Ô∏è ADVERTENCIA: Hay contextos con may√∫sculas - deber√≠a estar todo en min√∫sculas\n",
      "Ejemplos: [(('poder', 'Judicial.Por'), defaultdict(<class 'float'>, {'su': 1.0})), (('Judicial.Por', 'su'), defaultdict(<class 'float'>, {'parte': 1.0})), (('ganar√°n', 'S/20'), defaultdict(<class 'float'>, {'milcongreso': 1.0})), (('S/20', 'milcongreso'), defaultdict(<class 'float'>, {':': 1.0})), (('de', 'Hu√°nuco.Ante'), defaultdict(<class 'float'>, {'un': 1.0}))]\n",
      "\n",
      "üîç AN√ÅLISIS DE CONTEXTOS HU√âRFANOS\n",
      "======================================================================\n",
      "Contextos con may√∫sculas: 60099\n",
      "‚ö†Ô∏è ADVERTENCIA: Hay contextos con may√∫sculas - deber√≠a estar todo en min√∫sculas\n",
      "Ejemplos: [(('poder', 'Judicial.Por'), defaultdict(<class 'float'>, {'su': 1.0})), (('Judicial.Por', 'su'), defaultdict(<class 'float'>, {'parte': 1.0})), (('ganar√°n', 'S/20'), defaultdict(<class 'float'>, {'milcongreso': 1.0})), (('S/20', 'milcongreso'), defaultdict(<class 'float'>, {':': 1.0})), (('de', 'Hu√°nuco.Ante'), defaultdict(<class 'float'>, {'un': 1.0}))]\n",
      "\n",
      "üîç AN√ÅLISIS DE CONTEXTOS HU√âRFANOS\n",
      "======================================================================\n",
      "Contextos con solo 1 opci√≥n: 1624714 (71.4%)\n",
      "Total de contextos: 2274244\n",
      "\n",
      "üîç VERIFICAR COBERTURA DE PALABRAS COMUNES\n",
      "======================================================================\n",
      "Palabras comunes y sus contextos:\n",
      "   'el': aparece en 15397 contextos como w1, 18322 como w2\n",
      "Contextos con solo 1 opci√≥n: 1624714 (71.4%)\n",
      "Total de contextos: 2274244\n",
      "\n",
      "üîç VERIFICAR COBERTURA DE PALABRAS COMUNES\n",
      "======================================================================\n",
      "Palabras comunes y sus contextos:\n",
      "   'el': aparece en 15397 contextos como w1, 18322 como w2\n",
      "   'la': aparece en 17316 contextos como w1, 17186 como w2\n",
      "   'de': aparece en 40272 contextos como w1, 34579 como w2\n",
      "   'la': aparece en 17316 contextos como w1, 17186 como w2\n",
      "   'de': aparece en 40272 contextos como w1, 34579 como w2\n",
      "   'en': aparece en 13436 contextos como w1, 32367 como w2\n",
      "   'que': aparece en 17844 contextos como w1, 19840 como w2\n",
      "\n",
      "üîç AN√ÅLISIS DE PUNTUACI√ìN\n",
      "======================================================================\n",
      "Contextos que terminan en puntuaci√≥n:\n",
      "   'en': aparece en 13436 contextos como w1, 32367 como w2\n",
      "   'que': aparece en 17844 contextos como w1, 19840 como w2\n",
      "\n",
      "üîç AN√ÅLISIS DE PUNTUACI√ìN\n",
      "======================================================================\n",
      "Contextos que terminan en puntuaci√≥n:\n",
      "   Contextos (*, '.'): 44928\n",
      "      ‚Üí Palabras m√°s comunes despu√©s: [(None, 44864.92274901332), (\"''\", 38.83237590897667), (')', 17.79758016083059)]\n",
      "   Contextos (*, ','): 67851\n",
      "   Contextos (*, '.'): 44928\n",
      "      ‚Üí Palabras m√°s comunes despu√©s: [(None, 44864.92274901332), (\"''\", 38.83237590897667), (')', 17.79758016083059)]\n",
      "   Contextos (*, ','): 67851\n",
      "      ‚Üí Palabras m√°s comunes despu√©s: [('el', 2864.848739435402), ('que', 2789.492609407994), ('en', 2435.1827049280473)]\n",
      "   Contextos (*, '!'): 2032\n",
      "      ‚Üí Palabras m√°s comunes despu√©s: [(None, 1431.158650671142), ('‚Äù', 271.73496894533184), ('!', 64.14088102391997)]\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DIAGN√ìSTICO COMPLETADO\n",
      "======================================================================\n",
      "      ‚Üí Palabras m√°s comunes despu√©s: [('el', 2864.848739435402), ('que', 2789.492609407994), ('en', 2435.1827049280473)]\n",
      "   Contextos (*, '!'): 2032\n",
      "      ‚Üí Palabras m√°s comunes despu√©s: [(None, 1431.158650671142), ('‚Äù', 271.73496894533184), ('!', 64.14088102391997)]\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DIAGN√ìSTICO COMPLETADO\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Diagn√≥stico 1: Verificar contextos con None\n",
    "print(\"üîç AN√ÅLISIS DE CONTEXTOS CON PADDING (None)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "none_contexts = [(k, len(v)) for k, v in model_global.items() if None in k]\n",
    "print(f\"Total de contextos con None: {len(none_contexts)}\")\n",
    "print(f\"Contextos de inicio (None, None): {len(model_global.get((None, None), {}))}\")\n",
    "\n",
    "# Mostrar algunos ejemplos\n",
    "if (None, None) in model_global:\n",
    "    print(f\"\\nPalabras que pueden iniciar una oraci√≥n:\")\n",
    "    inicio_words = sorted(model_global[(None, None)].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for word, prob in inicio_words:\n",
    "        print(f\"   {word:>15}: {prob:.4f}\")\n",
    "\n",
    "# Diagn√≥stico 2: Verificar problemas de may√∫sculas\n",
    "print(\"\\nüîç AN√ÅLISIS DE MAY√öSCULAS/MIN√öSCULAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Buscar si hay contextos con palabras en may√∫sculas (no deber√≠a haber)\n",
    "uppercase_contexts = [(k, v) for k, v in model_global.items() \n",
    "                     if any(w and isinstance(w, str) and w[0].isupper() for w in k)]\n",
    "\n",
    "print(f\"Contextos con may√∫sculas: {len(uppercase_contexts)}\")\n",
    "if len(uppercase_contexts) > 0:\n",
    "    print(\"‚ö†Ô∏è ADVERTENCIA: Hay contextos con may√∫sculas - deber√≠a estar todo en min√∫sculas\")\n",
    "    print(\"Ejemplos:\", list(uppercase_contexts)[:5])\n",
    "else:\n",
    "    print(\"‚úÖ Correcto: Todos los contextos est√°n en min√∫sculas\")\n",
    "\n",
    "# Diagn√≥stico 3: Verificar contextos hu√©rfanos (con 1 sola opci√≥n)\n",
    "print(\"\\nüîç AN√ÅLISIS DE CONTEXTOS HU√âRFANOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "orphan_contexts = [(k, v) for k, v in model_global.items() if len(v) == 1]\n",
    "print(f\"Contextos con solo 1 opci√≥n: {len(orphan_contexts)} ({len(orphan_contexts)/len(model_global)*100:.1f}%)\")\n",
    "print(f\"Total de contextos: {len(model_global)}\")\n",
    "\n",
    "# Diagn√≥stico 4: Verificar cobertura de palabras comunes\n",
    "print(\"\\nüîç VERIFICAR COBERTURA DE PALABRAS COMUNES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "common_words = [\"el\", \"la\", \"de\", \"en\", \"que\", \"y\", \"a\", \"los\", \"del\", \"las\"]\n",
    "print(\"Palabras comunes y sus contextos:\")\n",
    "\n",
    "for word in common_words[:5]:\n",
    "    # Contar en cu√°ntos contextos aparece esta palabra\n",
    "    as_w1 = sum(1 for k in model_global.keys() if k[0] == word)\n",
    "    as_w2 = sum(1 for k in model_global.keys() if k[1] == word)\n",
    "    print(f\"   '{word}': aparece en {as_w1} contextos como w1, {as_w2} como w2\")\n",
    "\n",
    "# Diagn√≥stico 5: Verificar puntuaci√≥n\n",
    "print(\"\\nüîç AN√ÅLISIS DE PUNTUACI√ìN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "punctuation = ['.', ',', '!', '?', ';', ':']\n",
    "print(\"Contextos que terminan en puntuaci√≥n:\")\n",
    "\n",
    "for punct in punctuation[:3]:\n",
    "    contexts_ending = [(k, v) for k, v in model_global.items() if k[1] == punct]\n",
    "    if contexts_ending:\n",
    "        print(f\"   Contextos (*, '{punct}'): {len(contexts_ending)}\")\n",
    "        # Mostrar qu√© sigue despu√©s de la puntuaci√≥n\n",
    "        next_words = {}\n",
    "        for k, v in contexts_ending:\n",
    "            for w3, prob in v.items():\n",
    "                next_words[w3] = next_words.get(w3, 0) + prob\n",
    "        top_next = sorted(next_words.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        print(f\"      ‚Üí Palabras m√°s comunes despu√©s: {top_next}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ DIAGN√ìSTICO COMPLETADO\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298ef0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Resumen de Problemas Corregidos\n",
    "\n",
    "### ‚úÖ **Problemas identificados y solucionados:**\n",
    "\n",
    "1. **Fallbacks incorrectos en `sample_next`**:\n",
    "   - ‚ùå Antes: Buscaba contextos `(None, w2)` y `(w1, None)` que no existen\n",
    "   - ‚úÖ Ahora: Busca contextos reales que terminen en `w2` y usa inicio de oraci√≥n como fallback\n",
    "\n",
    "2. **Terminaci√≥n prematura de oraciones**:\n",
    "   - ‚ùå Antes: Se deten√≠a al primer `None` sin intentar continuar\n",
    "   - ‚úÖ Ahora: Intenta continuar con nuevo contexto antes de rendirse\n",
    "\n",
    "3. **Limpieza de espacios antes de puntuaci√≥n**:\n",
    "   - ‚ùå Antes: Generaba \" .\" en lugar de \".\"\n",
    "   - ‚úÖ Ahora: Elimina espacios antes de puntuaci√≥n autom√°ticamente\n",
    "\n",
    "4. **Manejo robusto de casos edge**:\n",
    "   - ‚úÖ Contador para evitar loops infinitos\n",
    "   - ‚úÖ Mejor manejo de contextos no vistos\n",
    "   - ‚úÖ Validaci√≥n consistente de min√∫sculas en seeds\n",
    "\n",
    "### üéØ **Mejoras adicionales implementadas:**\n",
    "\n",
    "- Validaci√≥n autom√°tica del modelo\n",
    "- Diagn√≥stico de problemas comunes\n",
    "- Verificaci√≥n de probabilidades\n",
    "- An√°lisis de cobertura de palabras\n",
    "\n",
    "### üí° **Recomendaciones:**\n",
    "\n",
    "1. **Tama√±o del corpus**: Tu modelo ser√° mejor con m√°s datos\n",
    "2. **Smoothing**: Considera agregar Laplace smoothing para contextos no vistos\n",
    "3. **Vocabulario**: Filtra palabras muy raras (< 5 apariciones) para reducir sparsity\n",
    "4. **Modelo m√°s avanzado**: Considera 4-gramas o modelos neurales (LSTM/Transformer) para mejor calidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f96db8",
   "metadata": {},
   "source": [
    "## üìä An√°lisis: Combinaciones m√°s frecuentes (Trigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "691e15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_most_frequent_trigrams(tokenized_sentences: List[List[str]], top_n: int = 20, exclude_padding: bool = True):\n",
    "    \"\"\"\n",
    "    Identifica los trigramas m√°s frecuentes en el corpus.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: Lista de oraciones tokenizadas\n",
    "        top_n: N√∫mero de trigramas a retornar\n",
    "        exclude_padding: Si True, excluye trigramas que contienen None (padding)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de tuplas ((w1, w2, w3), frecuencia)\n",
    "    \"\"\"\n",
    "    trigram_counts = Counter()\n",
    "    \n",
    "    for sent in tokenized_sentences:\n",
    "        for w1, w2, w3 in trigrams(sent, pad_left=True, pad_right=True):\n",
    "            # Opcionalmente excluir trigramas con padding\n",
    "            if exclude_padding and (w1 is None or w2 is None or w3 is None):\n",
    "                continue\n",
    "            trigram_counts[(w1, w2, w3)] += 1\n",
    "    \n",
    "    return trigram_counts.most_common(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIGRAMAS M√ÅS FRECUENTES - CORPUS GLOBAL\n"
     ]
    }
   ],
   "source": [
    "print(\"TRIGRAMAS M√ÅS FRECUENTES - CORPUS GLOBAL\")\n",
    "\n",
    "top_trigrams_global = get_most_frequent_trigrams(corpus_global[\"_GLOBAL\"], top_n=30)\n",
    "\n",
    "for i, ((w1, w2, w3), freq) in enumerate(top_trigrams_global, 1):\n",
    "    w1_str = f\"'{w1}'\" if w1 else \"None\"\n",
    "    w2_str = f\"'{w2}'\" if w2 else \"None\"\n",
    "    w3_str = f\"'{w3}'\" if w3 else \"None\"\n",
    "    \n",
    "    print(f\"{i:2}. ({w1_str:>15}, {w2_str:>15}, {w3_str:>15}) ‚Üí {freq:>5} veces\")\n",
    "\n",
    "print(f\"\\nTotal de trigramas √∫nicos: {len(set(trigrams(sum(corpus_global['_GLOBAL'], []), pad_left=True, pad_right=True)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde73b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar por categor√≠a\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRIGRAMAS M√ÅS FRECUENTES POR CATEGOR√çA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for categoria in [\"Deportes\", \"Pol√≠tica\", \"Econom√≠a\"]:\n",
    "    if categoria in corpus_por_categoria:\n",
    "        print(f\"\\nüìÇ {categoria.upper()}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        top_trigrams = get_most_frequent_trigrams(\n",
    "            corpus_por_categoria[categoria], \n",
    "            top_n=10, \n",
    "            exclude_padding=True\n",
    "        )\n",
    "        \n",
    "        for i, ((w1, w2, w3), freq) in enumerate(top_trigrams, 1):\n",
    "            print(f\"  {i:2}. ({w1}, {w2}, {w3}) ‚Üí {freq} veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21d057",
   "metadata": {},
   "source": [
    "### Visualizaci√≥n de trigramas frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91651eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_trigrams(trigrams_list, title=\"Top Trigramas\", top_n=15):\n",
    "    \"\"\"\n",
    "    Visualiza los trigramas m√°s frecuentes en un gr√°fico de barras.\n",
    "    \"\"\"\n",
    "    # Tomar solo los top_n\n",
    "    trigrams_list = trigrams_list[:top_n]\n",
    "    \n",
    "    # Preparar datos\n",
    "    labels = [f\"{w1} {w2} {w3}\" for (w1, w2, w3), _ in trigrams_list]\n",
    "    frequencies = [freq for _, freq in trigrams_list]\n",
    "    \n",
    "    # Crear gr√°fico\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    bars = ax.barh(range(len(labels)), frequencies, color='steelblue', alpha=0.8)\n",
    "    \n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_yticklabels(labels, fontsize=10)\n",
    "    ax.set_xlabel('Frecuencia', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Agregar valores al final de las barras\n",
    "    for i, (bar, freq) in enumerate(zip(bars, frequencies)):\n",
    "        ax.text(freq + 0.5, i, str(freq), va='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar trigramas del corpus global\n",
    "plot_top_trigrams(top_trigrams_global, title=\"Top 15 Trigramas - Corpus Global\", top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6701aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n entre categor√≠as\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Trigramas M√°s Frecuentes por Categor√≠a', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "categorias_plot = [\"Deportes\", \"Pol√≠tica\", \"Econom√≠a\", \"Internacional\"]\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, categoria in enumerate(categorias_plot):\n",
    "    if categoria in corpus_por_categoria:\n",
    "        top_cat = get_most_frequent_trigrams(\n",
    "            corpus_por_categoria[categoria], \n",
    "            top_n=10, \n",
    "            exclude_padding=True\n",
    "        )\n",
    "        \n",
    "        labels = [f\"{w1} {w2} {w3}\" for (w1, w2, w3), _ in top_cat]\n",
    "        frequencies = [freq for _, freq in top_cat]\n",
    "        \n",
    "        ax = axes_flat[idx]\n",
    "        bars = ax.barh(range(len(labels)), frequencies, color='coral', alpha=0.7)\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        ax.set_yticklabels(labels, fontsize=9)\n",
    "        ax.set_xlabel('Frecuencia', fontsize=10)\n",
    "        ax.set_title(f'üìÇ {categoria}', fontsize=12, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Valores en las barras\n",
    "        for i, (bar, freq) in enumerate(zip(bars, frequencies)):\n",
    "            ax.text(freq + 0.3, i, str(freq), va='center', fontsize=8)\n",
    "    else:\n",
    "        axes_flat[idx].text(0.5, 0.5, f'Categor√≠a \"{categoria}\"\\nno disponible', \n",
    "                           ha='center', va='center', transform=axes_flat[idx].transAxes)\n",
    "        axes_flat[idx].set_xticks([])\n",
    "        axes_flat[idx].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f8ab6",
   "metadata": {},
   "source": [
    "### An√°lisis adicional: Bigramas m√°s frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366562a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "def get_most_frequent_bigrams(tokenized_sentences: List[List[str]], top_n: int = 20, exclude_padding: bool = True):\n",
    "    \"\"\"\n",
    "    Identifica los bigramas m√°s frecuentes en el corpus.\n",
    "    \"\"\"\n",
    "    bigram_counts = Counter()\n",
    "    \n",
    "    for sent in tokenized_sentences:\n",
    "        for w1, w2 in bigrams(sent, pad_left=True, pad_right=True):\n",
    "            if exclude_padding and (w1 is None or w2 is None):\n",
    "                continue\n",
    "            bigram_counts[(w1, w2)] += 1\n",
    "    \n",
    "    return bigram_counts.most_common(top_n)\n",
    "\n",
    "# Analizar bigramas\n",
    "print(\"=\" * 70)\n",
    "print(\"BIGRAMAS M√ÅS FRECUENTES - CORPUS GLOBAL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "top_bigrams = get_most_frequent_bigrams(corpus_global[\"_GLOBAL\"], top_n=30, exclude_padding=True)\n",
    "\n",
    "for i, ((w1, w2), freq) in enumerate(top_bigrams, 1):\n",
    "    print(f\"{i:2}. ('{w1}', '{w2}') ‚Üí {freq:>5} veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bb7a7",
   "metadata": {},
   "source": [
    "### üìà Interpretaci√≥n de resultados\n",
    "\n",
    "Los trigramas m√°s frecuentes te muestran:\n",
    "1. **Patrones comunes** en el lenguaje period√≠stico espa√±ol\n",
    "2. **Frases t√≠picas** que se repiten en las noticias\n",
    "3. **Contextos espec√≠ficos** de cada categor√≠a (deportes, pol√≠tica, etc.)\n",
    "\n",
    "Esto es √∫til para:\n",
    "- Entender qu√© combinaciones de palabras son m√°s naturales\n",
    "- Mejorar la generaci√≥n de texto (el modelo aprende de estas frecuencias)\n",
    "- Identificar vocabulario caracter√≠stico de cada tem√°tica"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
