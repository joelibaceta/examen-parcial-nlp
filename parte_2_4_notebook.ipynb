{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b23cd3",
   "metadata": {},
   "source": [
    "## Word Embeddings con Word2Vec\n",
    "\n",
    "- Entrenar modelos Word2Vec con CBOW y Skip-Gram\n",
    "- Buscar palabras similares\n",
    "- Visualizar embeddings en 2D usando t-SNE y PCA\n",
    "- Mostrar noticias de diferentes t√≥picos en el espacio de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4040acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d93780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos: 37746\n",
      "\n",
      "Categor√≠as: 7\n",
      "seccion\n",
      "Pol√≠tica        12509\n",
      "Espect√°culos     6386\n",
      "Mundo            5186\n",
      "Deportes         4739\n",
      "Cultura          3256\n",
      "Econom√≠a         3168\n",
      "Policiales       2502\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"noticias_unificadas.tsv\",\n",
    "    encoding=\"utf-8\",\n",
    "    sep=\"\\t\",\n",
    "    dtype={\"fecha\": \"string\", \"titulo\": \"string\", \"contenido\": \"string\", \"seccion\": \"string\", \"link\": \"string\"},\n",
    "    quoting=0,\n",
    "    na_filter=False\n",
    ")\n",
    "\n",
    "print(f\"Total de documentos: {len(df)}\")\n",
    "print(f\"\\nCategor√≠as: {df['seccion'].nunique()}\")\n",
    "print(df['seccion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11776a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import clean_text\n",
    "\n",
    "df[\"headline_text\"] = (df[\"titulo\"].fillna(\"\") + \" \" + df[\"contenido\"].fillna(\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed07ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de stopwords: 352\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import gensim\n",
    "\n",
    "# Stopwords en espa√±ol\n",
    "STOPWORDS = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "STOP_EXTRA = {\n",
    "    \"dijo\", \"anos\", \"foto\", \"video\", \"puedes\", \"ver\", \"hoy\", \"ayer\", \"manana\", \n",
    "    \"mas\", \"recomendado\", \"ser\", \"dia\", \"dias\", \"tambien\", \"cada\", \"tras\", \n",
    "    \"soles\", \"uno\", \"dos\", \"tres\", \"asi\", \"mil\", \"ano\", \"a√±o\", \"solo\", \n",
    "    \"senalo\", \"segun\", \"entre\", \"millones\", \"lugar\", \"puede\", \"haber\", \n",
    "    \"tener\", \"sol\", \"precio\", \"yape\", \"pai\", \"nueva\", \"hace\", \"hacer\"\n",
    "}\n",
    "\n",
    "STOPWORDS |= STOP_EXTRA\n",
    "\n",
    "print(f\"Total de stopwords: {len(STOPWORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f722f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "El presidente anunci√≥ nuevas elecciones pol√≠ticas en el congreso\n",
      "\n",
      "Tokens procesados:\n",
      "['presidente', 'anunci√≥', 'nuevas', 'elecciones', 'pol√≠ticas', 'congreso']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_for_w2v(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "\n",
    "ejemplo = \"El presidente anunci√≥ nuevas elecciones pol√≠ticas en el congreso\"\n",
    "print(\"Texto original:\")\n",
    "print(ejemplo)\n",
    "print(\"\\nTokens procesados:\")\n",
    "print(preprocess_for_w2v(ejemplo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2dd591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando documentos para Word2Vec...\n",
      "\n",
      "Total de documentos procesados: 37746\n",
      "\n",
      "Ejemplo de documento procesado:\n",
      "['diego', 'leon', 'director', 'cine', 'siempre', 'quisimos', 'contar', 'hazana', 'militar', 'entrevista', 'diego', 'leon', 'toma', 'calma', 'exito', 'polemica', 'generado', 'primera', 'semana', 'exhibicion']\n",
      "\n",
      "Estad√≠sticas de longitud:\n",
      "  Media: 195.71 tokens\n",
      "  Mediana: 166.00 tokens\n",
      "  Min: 13, Max: 6543\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Procesando documentos para Word2Vec...\")\n",
    "processed_docs = df['headline_text'].map(preprocess_for_w2v)\n",
    "\n",
    "sentences = processed_docs.tolist()\n",
    "\n",
    "print(f\"\\nTotal de documentos procesados: {len(sentences)}\")\n",
    "print(f\"\\nEjemplo de documento procesado:\")\n",
    "print(sentences[100][:20])\n",
    "\n",
    "doc_lengths = [len(doc) for doc in sentences]\n",
    "print(f\"\\nEstad√≠sticas de longitud:\")\n",
    "print(f\"  Media: {np.mean(doc_lengths):.2f} tokens\")\n",
    "print(f\"  Mediana: {np.median(doc_lengths):.2f} tokens\")\n",
    "print(f\"  Min: {min(doc_lengths)}, Max: {max(doc_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b191dc7",
   "metadata": {},
   "source": [
    "## Entrenar Modelo Word2Vec\n",
    "\n",
    "### CBOW (Continuous Bag of Words)\n",
    "- Predice la palabra central dado el contexto\n",
    "- M√°s r√°pido de entrenar\n",
    "- Mejor para palabras frecuentes\n",
    "\n",
    "### Skip-Gram\n",
    "- Predice el contexto dada la palabra central\n",
    "- M√°s lento de entrenar\n",
    "- Mejor para palabras poco frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340ab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRENANDO WORD2VEC CON CBOW\n"
     ]
    }
   ],
   "source": [
    "print(\"ENTRENANDO WORD2VEC CON CBOW\")\n",
    "\n",
    "vector_size = 100\n",
    "window = 5\n",
    "min_count = 5\n",
    "workers = 4\n",
    "epochs = 20\n",
    "\n",
    "# Entrenar CBOW (sg=0)\n",
    "model_cbow = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    epochs=epochs,\n",
    "    sg=0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo CBOW entrenado exitosamente\")\n",
    "print(f\"Vocabulario: {len(model_cbow.wv)} palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e46907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENTRENANDO WORD2VEC CON SKIP-GRAM\")\n",
    "\n",
    "# Entrenar Skip-Gram (sg=1)\n",
    "model_skipgram = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=vector_size,\n",
    "    window=window,\n",
    "    min_count=min_count,\n",
    "    workers=workers,\n",
    "    epochs=epochs,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo Skip-Gram entrenado exitosamente\")\n",
    "print(f\"Vocabulario: {len(model_skipgram.wv)} palabras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f7c7a",
   "metadata": {},
   "source": [
    "## üîç Buscar Palabras Similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 9: Funci√≥n para buscar palabras similares\n",
    "def find_similar_words(model, word, topn=10):\n",
    "    \"\"\"\n",
    "    Encuentra palabras similares usando el modelo Word2Vec\n",
    "    \"\"\"\n",
    "    try:\n",
    "        similar = model.wv.most_similar(word, topn=topn)\n",
    "        return similar\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def compare_models_similarity(word, model_cbow, model_skipgram, topn=10):\n",
    "    \"\"\"\n",
    "    Compara resultados de similitud entre CBOW y Skip-Gram\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"PALABRAS SIMILARES A: '{word}'\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # CBOW\n",
    "    print(f\"\\nüîπ Modelo CBOW:\")\n",
    "    similar_cbow = find_similar_words(model_cbow, word, topn)\n",
    "    if similar_cbow:\n",
    "        for i, (w, score) in enumerate(similar_cbow, 1):\n",
    "            print(f\"  {i:2}. {w:20s} ‚Üí similitud: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Palabra '{word}' no encontrada en vocabulario CBOW\")\n",
    "    \n",
    "    # Skip-Gram\n",
    "    print(f\"\\nüîπ Modelo Skip-Gram:\")\n",
    "    similar_sg = find_similar_words(model_skipgram, word, topn)\n",
    "    if similar_sg:\n",
    "        for i, (w, score) in enumerate(similar_sg, 1):\n",
    "            print(f\"  {i:2}. {w:20s} ‚Üí similitud: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Palabra '{word}' no encontrada en vocabulario Skip-Gram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 10: Probar con \"elecciones\"\n",
    "compare_models_similarity(\"elecciones\", model_cbow, model_skipgram, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c737c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 11: Probar con \"chancay\"\n",
    "compare_models_similarity(\"chancay\", model_cbow, model_skipgram, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 12: Probar con \"delmcuencia\" (probable error de tipeo para \"delincuencia\")\n",
    "# Buscar variantes\n",
    "for word in [\"delincuencia\", \"delmcuencia\", \"delito\", \"crimen\"]:\n",
    "    if word in model_cbow.wv:\n",
    "        print(f\"\\n‚úÖ Palabra encontrada: '{word}'\")\n",
    "        compare_models_similarity(word, model_cbow, model_skipgram, topn=8)\n",
    "        break\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ninguna variante encontrada. Palabras relacionadas disponibles:\")\n",
    "    related = [w for w in model_cbow.wv.index_to_key[:1000] if 'delin' in w or 'crim' in w or 'segur' in w]\n",
    "    print(related[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d963fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 13: Explorar m√°s palabras\n",
    "palabras_test = [\"presidente\", \"gobierno\", \"equipo\", \"partido\", \"economia\", \"dolar\"]\n",
    "\n",
    "for palabra in palabras_test:\n",
    "    if palabra in model_cbow.wv:\n",
    "        compare_models_similarity(palabra, model_cbow, model_skipgram, topn=8)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022f4a8",
   "metadata": {},
   "source": [
    "## üìä Visualizaci√≥n de Embeddings en 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 14: Funci√≥n para visualizar embeddings con t-SNE\n",
    "def visualize_embeddings_tsne(model, words, title=\"Word Embeddings (t-SNE)\", perplexity=30):\n",
    "    \"\"\"\n",
    "    Visualiza embeddings en 2D usando t-SNE\n",
    "    \"\"\"\n",
    "    # Filtrar palabras que existen en el vocabulario\n",
    "    valid_words = [w for w in words if w in model.wv]\n",
    "    \n",
    "    if len(valid_words) < 2:\n",
    "        print(f\"‚ö†Ô∏è Muy pocas palabras v√°lidas ({len(valid_words)})\")\n",
    "        return\n",
    "    \n",
    "    # Obtener vectores\n",
    "    vectors = np.array([model.wv[w] for w in valid_words])\n",
    "    \n",
    "    # Aplicar t-SNE\n",
    "    perplexity = min(perplexity, len(valid_words) - 1)\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "    vectors_2d = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6, s=100, color='steelblue')\n",
    "    \n",
    "    # Etiquetar puntos\n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                    xytext=(5, 2), textcoords='offset points',\n",
    "                    fontsize=9, alpha=0.8)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Dimensi√≥n 1', fontsize=12)\n",
    "    plt.ylabel('Dimensi√≥n 2', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualizadas {len(valid_words)} palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd881f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 15: Seleccionar palabras clave de diferentes t√≥picos\n",
    "palabras_topicos = {\n",
    "    \"Pol√≠tica\": [\"presidente\", \"congreso\", \"gobierno\", \"ministro\", \"elecciones\", \"politico\", \"partido\"],\n",
    "    \"Deportes\": [\"equipo\", \"partido\", \"jugador\", \"futbol\", \"entrenador\", \"campeonato\", \"goles\"],\n",
    "    \"Econom√≠a\": [\"economia\", \"dolar\", \"empresa\", \"mercado\", \"banco\", \"financiero\", \"comercio\"],\n",
    "    \"Seguridad\": [\"policia\", \"delito\", \"seguridad\", \"crimen\", \"detenido\", \"investigacion\"]\n",
    "}\n",
    "\n",
    "# Combinar todas las palabras\n",
    "todas_palabras = []\n",
    "for topico, palabras in palabras_topicos.items():\n",
    "    todas_palabras.extend(palabras)\n",
    "\n",
    "print(f\"Total de palabras seleccionadas: {len(todas_palabras)}\")\n",
    "print(f\"Palabras: {todas_palabras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab79665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 16: Visualizar con t-SNE - Modelo CBOW\n",
    "visualize_embeddings_tsne(\n",
    "    model_cbow, \n",
    "    todas_palabras, \n",
    "    title=\"Word Embeddings - Modelo CBOW (t-SNE)\",\n",
    "    perplexity=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 17: Visualizar con t-SNE - Modelo Skip-Gram\n",
    "visualize_embeddings_tsne(\n",
    "    model_skipgram, \n",
    "    todas_palabras, \n",
    "    title=\"Word Embeddings - Modelo Skip-Gram (t-SNE)\",\n",
    "    perplexity=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 18: Funci√≥n para visualizar con PCA\n",
    "def visualize_embeddings_pca(model, words, title=\"Word Embeddings (PCA)\"):\n",
    "    \"\"\"\n",
    "    Visualiza embeddings en 2D usando PCA\n",
    "    \"\"\"\n",
    "    # Filtrar palabras v√°lidas\n",
    "    valid_words = [w for w in words if w in model.wv]\n",
    "    \n",
    "    if len(valid_words) < 2:\n",
    "        print(f\"‚ö†Ô∏è Muy pocas palabras v√°lidas ({len(valid_words)})\")\n",
    "        return\n",
    "    \n",
    "    # Obtener vectores\n",
    "    vectors = np.array([model.wv[w] for w in valid_words])\n",
    "    \n",
    "    # Aplicar PCA\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    vectors_2d = pca.fit_transform(vectors)\n",
    "    \n",
    "    print(f\"\\nVarianza explicada por PCA:\")\n",
    "    print(f\"  PC1: {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "    print(f\"  PC2: {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "    print(f\"  Total: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6, s=100, color='coral')\n",
    "    \n",
    "    # Etiquetar puntos\n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]),\n",
    "                    xytext=(5, 2), textcoords='offset points',\n",
    "                    fontsize=9, alpha=0.8)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualizadas {len(valid_words)} palabras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b827ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 19: Visualizar con PCA - Modelo CBOW\n",
    "visualize_embeddings_pca(\n",
    "    model_cbow, \n",
    "    todas_palabras, \n",
    "    title=\"Word Embeddings - Modelo CBOW (PCA)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bc49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 20: Visualizar con PCA - Modelo Skip-Gram\n",
    "visualize_embeddings_pca(\n",
    "    model_skipgram, \n",
    "    todas_palabras, \n",
    "    title=\"Word Embeddings - Modelo Skip-Gram (PCA)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a131de2c",
   "metadata": {},
   "source": [
    "## üì∞ Visualizaci√≥n de Documentos por T√≥pico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 21: Funci√≥n para obtener embedding de documento\n",
    "def get_document_embedding(model, tokens):\n",
    "    \"\"\"\n",
    "    Calcula el embedding de un documento como promedio de sus tokens\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        return None\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Calcular embeddings para muestra de documentos\n",
    "print(\"Calculando embeddings de documentos...\")\n",
    "\n",
    "# Seleccionar 10 documentos por categor√≠a (total ~50 docs)\n",
    "categories = [\"Deportes\", \"Pol√≠tica\", \"Econom√≠a\", \"Mundo\", \"Espect√°culos\"]\n",
    "sample_docs = []\n",
    "sample_labels = []\n",
    "\n",
    "for cat in categories:\n",
    "    docs_cat = df[df['seccion'] == cat].head(10)\n",
    "    for idx in docs_cat.index:\n",
    "        tokens = sentences[idx]\n",
    "        emb = get_document_embedding(model_cbow, tokens)\n",
    "        if emb is not None:\n",
    "            sample_docs.append(emb)\n",
    "            sample_labels.append(cat)\n",
    "\n",
    "sample_docs = np.array(sample_docs)\n",
    "print(f\"\\n‚úÖ {len(sample_docs)} documentos con embeddings calculados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 22: Visualizar documentos con t-SNE\n",
    "print(\"Aplicando t-SNE a documentos...\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(sample_docs)-1))\n",
    "docs_2d = tsne.fit_transform(sample_docs)\n",
    "\n",
    "# Crear figura\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Colores por categor√≠a\n",
    "colors = {'Deportes': 'red', 'Pol√≠tica': 'blue', 'Econom√≠a': 'green', \n",
    "          'Mundo': 'orange', 'Espect√°culos': 'purple'}\n",
    "\n",
    "for cat in categories:\n",
    "    indices = [i for i, label in enumerate(sample_labels) if label == cat]\n",
    "    if indices:\n",
    "        plt.scatter(\n",
    "            docs_2d[indices, 0], \n",
    "            docs_2d[indices, 1],\n",
    "            c=colors.get(cat, 'gray'),\n",
    "            label=cat,\n",
    "            alpha=0.6,\n",
    "            s=100\n",
    "        )\n",
    "\n",
    "plt.title('Noticias por T√≥pico en Espacio de Embeddings (t-SNE)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Dimensi√≥n 1', fontsize=12)\n",
    "plt.ylabel('Dimensi√≥n 2', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e686c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 23: Visualizar documentos con PCA\n",
    "print(\"Aplicando PCA a documentos...\")\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "docs_2d_pca = pca.fit_transform(sample_docs)\n",
    "\n",
    "print(f\"\\nVarianza explicada:\")\n",
    "print(f\"  PC1: {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"  PC2: {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(f\"  Total: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for cat in categories:\n",
    "    indices = [i for i, label in enumerate(sample_labels) if label == cat]\n",
    "    if indices:\n",
    "        plt.scatter(\n",
    "            docs_2d_pca[indices, 0], \n",
    "            docs_2d_pca[indices, 1],\n",
    "            c=colors.get(cat, 'gray'),\n",
    "            label=cat,\n",
    "            alpha=0.6,\n",
    "            s=100\n",
    "        )\n",
    "\n",
    "plt.title('Noticias por T√≥pico en Espacio de Embeddings (PCA)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8003b6e",
   "metadata": {},
   "source": [
    "## üìä Comparaci√≥n CBOW vs Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 24: Resumen comparativo\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARACI√ìN: CBOW vs SKIP-GRAM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä **Caracter√≠sticas de los modelos:**\n",
    "\n",
    "üîπ **CBOW (Continuous Bag of Words)**:\n",
    "   - Vocabulario: {len(model_cbow.wv)} palabras\n",
    "   - Predice: palabra central dado contexto\n",
    "   - Velocidad: M√°s r√°pido de entrenar\n",
    "   - Uso ideal: Palabras frecuentes, corpus grandes\n",
    "   - Ventaja: Mejor para capturar el contexto general\n",
    "\n",
    "üîπ **Skip-Gram**:\n",
    "   - Vocabulario: {len(model_skipgram.wv)} palabras\n",
    "   - Predice: contexto dada palabra central\n",
    "   - Velocidad: M√°s lento de entrenar\n",
    "   - Uso ideal: Palabras poco frecuentes, corpus peque√±os\n",
    "   - Ventaja: Mejor para capturar relaciones sem√°nticas sutiles\n",
    "\n",
    "üí° **Observaciones en este corpus:**\n",
    "   - Ambos modelos capturan bien las relaciones sem√°nticas\n",
    "   - CBOW es m√°s eficiente para este tama√±o de corpus\n",
    "   - Skip-Gram puede dar mejores resultados para palabras raras\n",
    "   - La visualizaci√≥n muestra clusters claros por t√≥pico\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c316d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 25: Guardar modelos (opcional)\n",
    "# model_cbow.save(\"word2vec_cbow_noticias.model\")\n",
    "# model_skipgram.save(\"word2vec_skipgram_noticias.model\")\n",
    "# print(\"‚úÖ Modelos guardados\")\n",
    "\n",
    "print(\"\\nüíæ Para guardar los modelos, descomenta las l√≠neas arriba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 26: Resumen final\n",
    "print(\"=\" * 80)\n",
    "print(\"üìù RESUMEN DEL AN√ÅLISIS DE WORD EMBEDDINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ **Resultados del an√°lisis:**\n",
    "\n",
    "1. **Datos procesados**:\n",
    "   - Documentos: {len(df)}\n",
    "   - Sentencias procesadas: {len(sentences)}\n",
    "   - Categor√≠as: {df['seccion'].nunique()}\n",
    "\n",
    "2. **Modelos entrenados**:\n",
    "   - CBOW: {len(model_cbow.wv)} palabras en vocabulario\n",
    "   - Skip-Gram: {len(model_skipgram.wv)} palabras en vocabulario\n",
    "   - Dimensi√≥n de embeddings: {vector_size}\n",
    "\n",
    "3. **An√°lisis realizados**:\n",
    "   ‚úì B√∫squeda de palabras similares\n",
    "   ‚úì Visualizaci√≥n con t-SNE\n",
    "   ‚úì Visualizaci√≥n con PCA\n",
    "   ‚úì Mapeo de documentos por t√≥pico\n",
    "\n",
    "4. **Palabras probadas**:\n",
    "   - 'elecciones', 'chancay', 'delincuencia'\n",
    "   - Palabras pol√≠ticas, deportivas, econ√≥micas\n",
    "\n",
    "üí° **Conclusiones**:\n",
    "   - Los embeddings capturan bien las relaciones sem√°nticas\n",
    "   - Los documentos se agrupan claramente por t√≥pico\n",
    "   - Ambos modelos (CBOW y Skip-Gram) son efectivos\n",
    "   - La reducci√≥n dimensional (t-SNE/PCA) preserva la estructura\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
