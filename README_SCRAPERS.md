# ğŸ“° Scrapers de Noticias - DocumentaciÃ³n

## ğŸ¯ Mejoras Implementadas

### 1. **Resiliencia ante Errores de ConexiÃ³n**
- âœ… Sistema de **reintentos automÃ¡ticos** con espera exponencial
- âœ… ConfiguraciÃ³n de `max_retries` (por defecto: 3 intentos)
- âœ… Espera exponencial: 5s â†’ 10s â†’ 20s
- âœ… No se detiene ante fallos temporales de red

### 2. **ReanudaciÃ³n AutomÃ¡tica**
- âœ… **Detecta automÃ¡ticamente** si ya existe un archivo TSV
- âœ… **Lee la Ãºltima fecha** procesada del archivo
- âœ… **ContinÃºa desde un dÃ­a anterior** a la Ãºltima fecha
- âœ… **Modo append** para no perder datos previos

### 3. **Manejo de Interrupciones**
- âœ… Captura `Ctrl+C` (KeyboardInterrupt)
- âœ… Guarda el progreso actual antes de salir
- âœ… Permite reanudar en la prÃ³xima ejecuciÃ³n

## ğŸš€ Uso

### Scraper de Peru21

```python
from scrapper_peru21 import Peru21Scrapper
from datetime import datetime

# Uso bÃ¡sico (con reanudaciÃ³n automÃ¡tica)
scraper = Peru21Scrapper()
scraper.extract_historical(
    output_file='noticias_peru21.tsv',
    resume=True  # ContinÃºa desde Ãºltima fecha si existe el archivo
)

# ConfiguraciÃ³n personalizada
scraper = Peru21Scrapper(
    max_workers=20,      # Hilos paralelos
    verbose=True,        # Mostrar progreso
    max_retries=5,       # Aumentar reintentos
    retry_delay=10       # Espera base mÃ¡s larga
)

scraper.extract_historical(
    start_date=datetime(2025, 1, 1),  # Fecha especÃ­fica
    output_file='noticias_peru21.tsv',
    max_empty_attempts=10,
    resume=True
)
```

### Scraper de Diario Correo

```python
from scrapper import NewsScrapper
from datetime import datetime

# Uso bÃ¡sico (con reanudaciÃ³n automÃ¡tica)
scraper = NewsScrapper()
scraper.extract_historical(
    output_file='noticias.tsv',
    resume=True
)

# ConfiguraciÃ³n personalizada
scraper = NewsScrapper(
    max_workers=10,
    verbose=True,
    max_retries=3,
    retry_delay=5
)

scraper.extract_historical(
    start_date=datetime(2025, 1, 1),
    output_file='noticias.tsv',
    max_empty_attempts=10,
    resume=True
)
```

## ğŸ“Š ParÃ¡metros Configurables

| ParÃ¡metro | DescripciÃ³n | Valor por defecto |
|-----------|-------------|-------------------|
| `max_workers` | Hilos paralelos para descargar noticias | Peru21: 20, Correo: 10 |
| `verbose` | Mostrar progreso en consola | `True` |
| `max_retries` | Reintentos por cada request fallido | `3` |
| `retry_delay` | Segundos de espera base (se duplica cada intento) | `5` |
| `start_date` | Fecha desde donde empezar (si `None`, usa hoy) | `None` |
| `output_file` | Archivo de salida TSV | `'noticias_*.tsv'` |
| `max_empty_attempts` | DÃ­as consecutivos sin noticias antes de parar | `10` |
| `resume` | Continuar desde Ãºltima fecha del archivo | `True` |

## ğŸ”„ Flujo de ReanudaciÃ³n

```
1. Script inicia
   â†“
2. Â¿Existe archivo TSV y resume=True?
   â”‚
   â”œâ”€ SÃ­ â†’ Lee Ãºltima fecha del archivo
   â”‚        ContinÃºa desde (Ãºltima_fecha - 1 dÃ­a)
   â”‚        Modo: APPEND
   â”‚
   â””â”€ No â†’ Comienza desde start_date (o hoy)
           Modo: WRITE
```

## ğŸ’¡ Ejemplos de Uso

### Escenario 1: Primera ejecuciÃ³n
```python
# El script descarga desde hoy hacia atrÃ¡s
scraper = Peru21Scrapper()
scraper.extract_historical(output_file='noticias.tsv')
```

### Escenario 2: InterrupciÃ³n y reanudaciÃ³n
```bash
# Primera ejecuciÃ³n (procesa hasta 2025-11-05)
$ python scrapper_peru21.py
ğŸ“° Descargadas: 150 noticias | Fecha actual: 2025-11-05
^C
â¸ï¸ Proceso interrumpido. Total: 150 noticias guardadas

# Segunda ejecuciÃ³n (continÃºa automÃ¡ticamente desde 2025-11-04)
$ python scrapper_peru21.py
ğŸ“… Ãšltima noticia encontrada: 2025-11-05
ğŸ”„ Reanudando desde: 2025-11-04
ğŸ“° Descargadas: 180 noticias | Fecha actual: 2025-11-04
```

### Escenario 3: Error de conexiÃ³n
```python
# El script reintenta automÃ¡ticamente
âš ï¸ Error de conexiÃ³n (intento 1/3). Esperando 5s...
âš ï¸ Error de conexiÃ³n (intento 2/3). Esperando 10s...
âœ… ConexiÃ³n recuperada
ğŸ“° Descargadas: 200 noticias | Fecha actual: 2025-11-03
```

## ğŸ“ Notas Importantes

1. **Pandas requerido**: Ahora se necesita `pandas` para leer fechas del archivo
   ```bash
   pip install pandas
   ```

2. **Archivos existentes**: Si quieres comenzar de cero, elimina el archivo `.tsv`
   ```bash
   rm noticias_peru21.tsv
   ```

3. **InterrupciÃ³n segura**: Usa `Ctrl+C` para detener de forma segura
   - Los datos se guardan incrementalmente
   - Puedes reanudar en cualquier momento

4. **Reintentos inteligentes**: 
   - Espera exponencial: 5s, 10s, 20s
   - Solo se aplica a errores de conexiÃ³n
   - Otros errores se registran y el script continÃºa

## ğŸ› SoluciÃ³n de Problemas

### Problema: El script no detecta la Ãºltima fecha
**SoluciÃ³n**: Verifica que el archivo TSV tenga el header correcto:
```tsv
fecha	titular	contenido	seccion	url
```

### Problema: Muchos errores de conexiÃ³n
**SoluciÃ³n**: Aumenta los parÃ¡metros de reintento:
```python
scraper = Peru21Scrapper(
    max_retries=5,
    retry_delay=10
)
```

### Problema: Quiero empezar de cero
**SoluciÃ³n**: 
```python
# OpciÃ³n 1: Eliminar el archivo
import os
os.remove('noticias.tsv')

# OpciÃ³n 2: Deshabilitar reanudaciÃ³n
scraper.extract_historical(resume=False)
```

## ğŸ“ˆ Rendimiento

- **Peru21**: ~20 hilos paralelos, ~1-2 noticias/segundo
- **Diario Correo**: ~10 hilos paralelos, ~0.5-1 noticias/segundo
- **Consumo de memoria**: MÃ­nimo (procesamiento incremental)
- **Almacenamiento**: ~1-2 KB por noticia

## âœ… Checklist de ValidaciÃ³n

- âœ… Reintentos automÃ¡ticos funcionando
- âœ… ReanudaciÃ³n desde Ãºltima fecha
- âœ… Manejo de Ctrl+C
- âœ… Modo append preserva datos anteriores
- âœ… Espera exponencial implementada
- âœ… Mensajes informativos en consola
